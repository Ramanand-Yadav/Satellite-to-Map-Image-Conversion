{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pix2PixGAN.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-ydVK17djkN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import torch \n",
        "import torchvision\n",
        "from torchvision import models, transforms, datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "GkRoOgHgefiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyper-parameters**\n",
        "\n",
        "\n",
        "*   Hyper-parameter suggested by the original paper\n",
        "\n"
      ],
      "metadata": {
        "id": "YplPDPJ1fMhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs=1 # suggested by the paper\n",
        "lr=0.0002 \n",
        "beta1=0.5\n",
        "beta2=0.999\n",
        "NUM_EPOCHS = 200\n",
        "ngpu = 1\n",
        "L1_lambda = 100\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "B4iWKGY5fbbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Import Dataset and Load them Into Batches**\n"
      ],
      "metadata": {
        "id": "2oyvttPcf-_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transform = transforms.Compose([\n",
        "                                     transforms.Resize((256, 512)),\n",
        "                                     transforms.CenterCrop((256, 512)),\n",
        "                                     transforms.RandomVerticalFlip(p=0.5),\n",
        "                                      transforms.ToTensor(),\n",
        "                                     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "dataset_train = datasets.ImageFolder(root = os.path.join(\"/content/drive/MyDrive/ColabWorkData/maps/maps\", \"train\"), transform=data_transform)\n",
        "dataset_val = datasets.ImageFolder(root = os.path.join(\"/content/drive/MyDrive/ColabWorkData/maps/maps\", \"val\"), transform=data_transform)\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=bs, shuffle=True, num_workers=0)\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=True, num_workers=0)\n"
      ],
      "metadata": {
        "id": "LCPc9qw-f5xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataloader_train))\n",
        "print(len(dataloader_val))"
      ],
      "metadata": {
        "id": "oX5f8TH8kXHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Define Function to show Image**"
      ],
      "metadata": {
        "id": "B9s-loVbmUyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(img, title=\"No title\", figsize=(5,5)):\n",
        "    img = img.numpy().transpose(1,2,0)\n",
        "    mean = np.array([0.5, 0.5, 0.5])\n",
        "    std = np.array([0.5, 0.5, 0.5])\n",
        "    \n",
        "    img = img * std + mean\n",
        "    np.clip(img, 0, 1)\n",
        "    \n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(img)\n",
        "    plt.title(title)"
      ],
      "metadata": {
        "id": "A72L1QbJmSgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images,_ = next(iter(dataloader_train))\n",
        "\n",
        "sample_sat = images[0][:,:,:256]\n",
        "sample_map = images[0][:,:,256:]"
      ],
      "metadata": {
        "id": "kZz6ieFkn8DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image(images[0], title=\"Satelite to Map\", figsize=(8, 8))"
      ],
      "metadata": {
        "id": "R84ggt0yoWrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image(img=sample_sat, title=\"Satelite Map\", figsize=(5, 5))"
      ],
      "metadata": {
        "id": "pDGPomr8pMZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image(img=sample_map,title=\"Map Image\", figsize=(5, 5))"
      ],
      "metadata": {
        "id": "QMb4hl4vpfWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images,_ = next(iter(dataloader_train))\n",
        "x = torchvision.utils.make_grid(images[:,:,:,:256], padding=10)\n",
        "y = torchvision.utils.make_grid(images[:,:,:,256:], padding=10)\n",
        "\n",
        "show_image(img=x, title=\"input Images\", figsize=(10, 10))\n",
        "show_image(img=y, title=\"output Images\", figsize=(10, 10))"
      ],
      "metadata": {
        "id": "KFRscM0YpohO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Functions to initialize model weights**\n"
      ],
      "metadata": {
        "id": "D3NV1j6Cr3Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "  name = m.__class__.__name__\n",
        "  if(name.find(\"Conv\") > -1 ):\n",
        "    nn.init.normal_(m.weight.data, 0.0, 0.02) #~N(mean=0.0, std=0.02)\n",
        "  elif(name.find(\"BatchNorm\") > -1 ): \n",
        "    nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "    nn.init.constant_(m.bias.data, 0.0)"
      ],
      "metadata": {
        "id": "9q_LYUzEr8Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.1 Generator - Architecture**\n",
        "\n",
        "\n",
        "*   The Architechture of u-net 256\n",
        "*   In 3 * 256 * 256 - Encoder C64-C128-C256-C512-C512-C512\n",
        "*   C512(Latent Space)\n",
        "*   Decoder DC1024-DC1024-DC1024-DC512-DC256-DC128-Out 3*256*256\n",
        "\n",
        "*  Reference: https://arxiv.org/pdf/1505.04597.pdf\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I4PHN-xItph9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    The Generator is a U-Net 256 with skip connections between Encoder and Decoder\n",
        "\"\"\"\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        \n",
        "        \"\"\"\n",
        "        ===== Encoder ======\n",
        "        \n",
        "        * Encoder has the following architecture:\n",
        "        0) Inp3 \n",
        "        1) C64 \n",
        "        2) Leaky, C128, Norm \n",
        "        3) Leaky, C256, Norm \n",
        "        4) Leaky, C512, Norm \n",
        "        5) Leaky, C512, Norm \n",
        "        6) Leaky, C512, Norm \n",
        "        7) Leaky, C512\n",
        "        \n",
        "        * The structure of 1 encoder block is:\n",
        "        1) LeakyReLU(prev layer)\n",
        "        2) Conv2D\n",
        "        3) BatchNorm\n",
        "        \n",
        "        Where Conv2D has kernel_size-4, stride=2, padding=1 for all layers\n",
        "        \"\"\"\n",
        "        self.encoder1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "        \n",
        "        self.encoder3 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        \n",
        "        self.encoder4 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512)\n",
        "        )\n",
        "        \n",
        "        self.encoder5 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512)\n",
        "        )\n",
        "        \n",
        "        self.encoder6 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512)\n",
        "        )\n",
        "        \n",
        "        self.encoder7 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        ===== Decoder =====\n",
        "        * Decoder has the following architecture:\n",
        "        1) ReLU(from latent space), DC512, Norm, Drop 0.5 - Residual\n",
        "        2) ReLU, DC512, Norm, Drop 0.5, Residual\n",
        "        3) ReLU, DC512, Norm, Drop 0.5, Residual\n",
        "        4) ReLU, DC256, Norm, Residual\n",
        "        5) ReLU, DC128, Norm, Residual\n",
        "        6) ReLU, DC64, Norm, Residual\n",
        "        7) ReLU, DC3, Tanh()\n",
        "        \n",
        "        * Note: only apply Dropout in the first 3 Decoder layers\n",
        "        \n",
        "        * The structure of each Decoder block is:\n",
        "        1) ReLU(from prev layer)\n",
        "        2) ConvTranspose2D\n",
        "        3) BatchNorm\n",
        "        4) Dropout\n",
        "        5) Skip connection\n",
        "        \n",
        "        Where ConvTranpose2D has kernel_size=4, stride=2, padding=1\n",
        "        \"\"\"\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        # skip connection in forward()\n",
        "        \n",
        "        self.decoder2 = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(in_channels=512*2, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        # skip connection in forward()\n",
        "\n",
        "        self.decoder3 = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(in_channels=512*2, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        # skip connection in forward()\n",
        "        \n",
        "        self.decoder4 = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(in_channels=512*2, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            #nn.Dropout(0.5)\n",
        "        )\n",
        "        \n",
        "        self.decoder5 = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(in_channels=256*2, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            #nn.Dropout(0.5)\n",
        "        )\n",
        "        \n",
        "        self.decoder6 = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(in_channels=128*2, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            #nn.Dropout(0.5)\n",
        "        )\n",
        "        \n",
        "        self.decoder7 = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(in_channels=64*2, out_channels=3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        e4 = self.encoder4(e3)\n",
        "        e5 = self.encoder5(e4)\n",
        "        e6 = self.encoder6(e5)\n",
        "        \n",
        "        latent_space = self.encoder7(e6)\n",
        "        \n",
        "        d1 = torch.cat([self.decoder1(latent_space), e6], dim=1)\n",
        "        d2 = torch.cat([self.decoder2(d1), e5], dim=1)\n",
        "        d3 = torch.cat([self.decoder3(d2), e4], dim=1)\n",
        "        d4 = torch.cat([self.decoder4(d3), e3], dim=1)\n",
        "        d5 = torch.cat([self.decoder5(d4), e2], dim=1)\n",
        "        d6 = torch.cat([self.decoder6(d5), e1], dim=1)\n",
        "        \n",
        "        out = self.decoder7(d6)\n",
        "        \n",
        "        return out        "
      ],
      "metadata": {
        "id": "XtG0WjHks7i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.2 Generator Initialization**"
      ],
      "metadata": {
        "id": "l9krGD8Av7GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_G = Generator(ngpu=1)\n",
        "\n",
        "if(device == \"cuda\" and ngpu > 1):\n",
        "    model_G = nn.DataParallel(model_G, list(range(ngpu)))\n",
        "    \n",
        "model_G.apply(weights_init)\n",
        "model_G.to(device)"
      ],
      "metadata": {
        "id": "40xW5gYfv0dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_G.to(device)\n",
        "#model_G(images[:,:,:,:256]).shape\n"
      ],
      "metadata": {
        "id": "7SVSKodpwFdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2.1 Discriminator - Architecture**\n",
        "*  PatchGan Discriminator with receptive field of each patch is rf = 70x70\n",
        "*  Structure is C64 (no norm, 4,2,1)-C128 (4,2,1)-C256 (4,2,1)-C512 (4,1,1) - Channel 1 (4,1,1), where C is Convolution - BatchNorm - LeakyReLU\n",
        "*  Activations are LeakyReLU(0.2)\n",
        "*  BatchNorm is applied to all layers but C64\n",
        "*  Last 2 layers has kernel_size=4, stride=1, padding=1"
      ],
      "metadata": {
        "id": "-K_V9_a1wcYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        \n",
        "        self.structure = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3*2, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.Conv2d(in_channels=64, out_channels= 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.structure(x)"
      ],
      "metadata": {
        "id": "_ZSAd6CywYiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2.2 Discriminator Initialization**"
      ],
      "metadata": {
        "id": "ovdjvdOVw3Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_D = Discriminator(ngpu=1)\n",
        "\n",
        "if(device == \"cuda\" and ngpu>1):\n",
        "    model_D = torch.DataParallel(model_D, list(rang(ngpu)))\n",
        "    \n",
        "model_D.apply(weights_init)\n",
        "model_D.to(device)"
      ],
      "metadata": {
        "id": "bchRmCkuw06V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "PbqL2TZbxUy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out1 = model_D(torch.cat([images[:,:,:,:256].to(device), images[:,:,:,256:].to(device)], dim=1)).to(device)\n",
        "out2 = torch.ones(size=out1.shape, dtype=torch.float, device=device)\n",
        "\n",
        "print(out1.shape)\n",
        "print(criterion(out1, out2))"
      ],
      "metadata": {
        "id": "Qafp3J8kxE56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Training process**\n",
        "*  We train Generator 2 times for each time we train Discriminator\n",
        "\n",
        "*  Let  be the input satellite image and  be the ground-truth map image. The objective is to train the U-Net Generator  so that it can translate satellite image to a map image as realistic as possible."
      ],
      "metadata": {
        "id": "om-_ASWyxY6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizerD = optim.Adam(model_D.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "optimizerG = optim.Adam(model_G.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "NUM_EPOCHS=171\n",
        "model_D.to(device)\n",
        "model_G.to(device)\n",
        "print()"
      ],
      "metadata": {
        "id": "X5aDe1foxXHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L1_lambda = 100\n",
        "NUM_EPOCHS= 50\n"
      ],
      "metadata": {
        "id": "vUPFwBSWxjyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS+1):\n",
        "    print(f\"Training epoch {epoch+1}\")\n",
        "    for images,_ in iter(dataloader_train):\n",
        "        # ========= Train Discriminator ===========\n",
        "        # Train on real data\n",
        "        # Maximize log(D(x,y)) <- maximize D(x,y)\n",
        "        model_D.zero_grad()\n",
        "        \n",
        "        inputs = images[:,:,:,:256].to(device) # input image data\n",
        "        targets = images[:,:,:,256:].to(device) # real targets data\n",
        "        \n",
        "        real_data = torch.cat([inputs, targets], dim=1).to(device)\n",
        "        outputs = model_D(real_data) # label \"real\" data\n",
        "        labels = torch.ones(size = outputs.shape, dtype=torch.float, device=device)\n",
        "        \n",
        "        lossD_real = 0.5 * criterion(outputs, labels) # divide the objective by 2 -> slow down D\n",
        "        lossD_real.backward()\n",
        "        \n",
        "        # Train on fake data\n",
        "        # Maximize log(1-D(x,G(x))) <- minimize D(x,G(x))\n",
        "        gens = model_G(inputs).detach()\n",
        "\n",
        "        fake_data = torch.cat([inputs, gens], dim=1) # generated image data\n",
        "        outputs = model_D(fake_data)\n",
        "        labels = torch.zeros(size = outputs.shape, dtype=torch.float, device=device) # label \"fake\" data\n",
        "        \n",
        "        lossD_fake = 0.5 * criterion(outputs, labels) # divide the objective by 2 -> slow down D\n",
        "        lossD_fake.backward()\n",
        "        \n",
        "        optimizerD.step()\n",
        "        \n",
        "        # ========= Train Generator x2 times ============\n",
        "        # maximize log(D(x, G(x)))\n",
        "        for i in range(2):\n",
        "            model_G.zero_grad()\n",
        "            \n",
        "            gens = model_G(inputs)\n",
        "            \n",
        "            gen_data = torch.cat([inputs, gens], dim=1) # concatenated generated data\n",
        "            outputs = model_D(gen_data)\n",
        "            labels = torch.ones(size = outputs.shape, dtype=torch.float, device=device)\n",
        "            \n",
        "            lossG = criterion(outputs, labels) + L1_lambda * torch.abs(gens-targets).sum()\n",
        "            lossG.backward()\n",
        "            optimizerG.step()\n",
        "            \n",
        "    if(epoch%5==0):\n",
        "        torch.save(model_G, \"./sat2mapGen_v1.3.pth\")\n",
        "        torch.save(model_D, \"./sat2mapDisc_v1.3.pth\")\n",
        "    \n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "jVhDUO_Jxuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Result**"
      ],
      "metadata": {
        "id": "7cFShUbnyLIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model_G = torch.load(\"./sat2mapGen_v1.2.pth\")\n",
        "#model_G.apply(weights_init)\n",
        "\n",
        "test_imgs,_ = next(iter(dataloader_val))\n",
        "\n",
        "satellite = test_imgs[:,:,:,:256].to(device)\n",
        "maps = test_imgs[:,:,:,256:].to(device)\n",
        "\n",
        "gen = model_G(satellite)\n",
        "#gen = gen[0]\n",
        "\n",
        "satellite = satellite.detach().cpu()\n",
        "gen = gen.detach().cpu()\n",
        "maps = maps.detach().cpu()\n",
        "\n",
        "show_image(torchvision.utils.make_grid(satellite, padding=10), title=\"Satellite\", figsize=(50,50))\n",
        "show_image(torchvision.utils.make_grid(gen, padding=10), title=\"Generated\", figsize=(50,50))\n",
        "#show_image(torchvision.utils.make_grid(maps, padding=10), title=\"Expected Output\", figsize=(50,50))"
      ],
      "metadata": {
        "id": "BTe1zGhyyEtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show_image(img=maps, title=\"Expected Output\", figsize=(5,5))"
      ],
      "metadata": {
        "id": "8T-ojrZmzwDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model_D(torch.cat([satellite[6,:,:,:].reshape(1,3,256,256).to(device), gen[6,:,:,:].reshape(1,3,256,256).to(device)], dim=1)))"
      ],
      "metadata": {
        "id": "YILaghTqzxBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_G, \"./sat2mapGen_v1.1.pth\")\n",
        "torch.save(model_D, \"./sat2mapDisc_v1.1.pth\")"
      ],
      "metadata": {
        "id": "TrEpgcqmz1RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_G = torch.load(\"./sat2mapGen_v1.1.pth\")"
      ],
      "metadata": {
        "id": "mX1j_StYz5gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Random Experiment zone (trivial stuffs)**"
      ],
      "metadata": {
        "id": "e7NOoaHLz80-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = images[:,:,:,:256]\n",
        "target = images[:,:,:,256:]\n",
        "\n",
        "print(inp.shape)\n",
        "print(target.shape)"
      ],
      "metadata": {
        "id": "XGiO5E0Mz6-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_real = torch.cat([inp, target], dim=1)\n",
        "print(data_real.shape)"
      ],
      "metadata": {
        "id": "mGIyDt6G0DF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformmm = transforms.Compose([\n",
        "    transforms.RandomVerticalFlip(p=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "3cwvTwbS0GTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasettt = datasets.ImageFolder(root=os.path.join(\"/content/drive/MyDrive/ColabWorkData/maps/maps\",\"train\"), transform=transformmm)"
      ],
      "metadata": {
        "id": "Y5Ng0w5U0Irl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaderrrr = torch.utils.data.DataLoader(datasettt, batch_size=1, shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "TFBAmvAa0K69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgsss,_ = next(iter(dataloaderrrr))\n",
        "show_image(imgsss[0,:,:,:])"
      ],
      "metadata": {
        "id": "C2RWQWID0NO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"TransposeConv2d\"\n",
        "print(name.find(\"Conv\"))"
      ],
      "metadata": {
        "id": "xO_-VOjz0Pcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.ones(size=(8,3,5,5))\n",
        "b = torch.zeros(size=(8,3,5,5))\n",
        "\n",
        "torch.abs(a-b).sum()"
      ],
      "metadata": {
        "id": "N91Y2lpd0Rq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wEs-s5l4zvw0"
      }
    }
  ]
}